{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div style=\"color:#ffffff;\n",
        "          font-size:50px;\n",
        "          font-style:italic;\n",
        "          text-align:left;\n",
        "          font-family: 'Lucida Bright';\n",
        "          background:#4686C8;\">\n",
        "  \t&nbsp; Word2vec CBOW from scratch\n",
        "</div>\n",
        "<br>   \n",
        "<div style=\"\n",
        "          font-size:20px;\n",
        "          text-align:left;\n",
        "          font-family: 'Palatino';\n",
        "          \">\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project: Embedding with word2vec & CBOW using PyTorch and Python<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Author: George Barrinuevo<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Date: 07/06/2025<br>\n",
        "</div>"
      ],
      "metadata": {
        "id": "GiOsrogYgyFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><div style=\"color:#ffffff;\n",
        "          font-size:30px;\n",
        "          font-style:italic;\n",
        "          text-align:left;\n",
        "          font-family: 'Lucida Bright';\n",
        "          background:#4686C8;\">\n",
        "  \t      &nbsp; Project Notes\n",
        "</div>\n",
        "<div style=\"\n",
        "          font-size:16px;\n",
        "          text-align:left;\n",
        "          font-family: 'Cambria';\">\n",
        "    \n",
        "<b>My Thoughts</b>\n",
        "- This is an example of implementing word2vec embedding with CBOW (Continuous Bag of Words) created from scratch. The purpose of this script is for educational purposes.\n",
        "- The script uses PyTorch and Python.\n",
        "\n",
        "<b>Technical Details</b>\n",
        "- The word2vec is an embedding method that determines how input text is prepared for training a model. It has two variants: CBOW and Skip-Gram. This script implements the CBOW version of word2vec.\n",
        "- The CBOW divides text in to target and context. The target is just a single word. If the window size is 2, the context is a list of words that is 2 words just before the target and 2 words just after the target. The input is the context and the truth to be predicted is the target.\n",
        "- The accuracy of this model depends on the size of the text corpus, number of epochs used in training, CPU/GPU power, and etc. The bigger or larger (e.g. larger text corpus or CPU/GPU) the better.\n",
        "- A vocabulary is created which maps words to an index number, also known as the Token ID. The word_to_ix variable takes a string word and outputs it's corresponding Token ID. The ix_to_word takes a Token ID and outputs the corresponding string word.\n",
        "- The EMDEDDING_DIM value can be any number you want. This is similar to 'features' such as a King and Queen share a common feature which is 'royalty'. A smaller EMDEDDING_DIM value captures less features/complexity but requires less CPU/GPU power. A larger value captures more features/complexity but needs more processing power. If you need more accurcy, try increasing this value.\n",
        "- The WINDOW_SIZE is the number of words to include in the text. If this value is 4, then it includes the 4 words before and 4 words after the target word.\n"
      ],
      "metadata": {
        "id": "7bx_e9dUDhp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "\n",
        "# After installing these packages, restart the kernel and re-run this notebook."
      ],
      "metadata": {
        "id": "s9QJMf3RQHYp",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "WINDOW_SIZE = 4\n",
        "EMDEDDING_DIM = 100\n",
        "\n",
        "raw_text = '''Among the vicissitudes incident to life, no event could have filled me with greater anxieties than that of which the notification\n",
        "was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I\n",
        "can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with\n",
        "an immutable decision, as the asylum of my declining years â€” a retreat which was rendered every day more necessary as well as more dear to me by\n",
        "the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time. On the other hand,\n",
        "the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced\n",
        "of her citizens a distrustful scrutiny into his qualifications, could not but overwhelm with despondence one who (inheriting inferior endowments\n",
        "from nature and unpracticed in the duties of civil administration) ought to be peculiarly conscious of his own deficiencies. In this conflict of\n",
        "emotions, all I dare aver is that it has been my faithful study to collect my duty from a just appreciation of every circumstance by which it might\n",
        "be affected. All I dare hope is that if, in executing this task, I have been too much swayed by a grateful remembrance of former instances or by an\n",
        "affectionate sensibility to this transcendent proof of the confidence of my fellow citizens, and have thence too little consulted my incapacity as\n",
        "well as disinclination for the weighty and untried cares before me, my error will be palliated by the motives which mislead me, and its consequences\n",
        "be judged by my country with some share of the partiality in which they originated. Such being the impressions under which I have, in obedience to\n",
        "the public summons, repaired to the present station, it would be peculiarly improper to omit in this first official act my fervent supplications to\n",
        "that Almighty Being who rules over the universe, who presides in the councils of nations, and whose providential aids can supply every human defect,\n",
        "that His benediction may consecrate to the liberties and happiness of the people of the United States a Government instituted by themselves for these\n",
        "essential purposes, and may enable every instrument employed in its administration to execute with success the functions allotted to his charge. In\n",
        "tendering this homage to the Great Author of every public and private good, I assure myself that it expresses your sentiments not less than my own,\n",
        "nor those of my fellow-citizens at large less than either. No people can be bound to acknowledge and adore the Invisible Hand which conducts the\n",
        "affairs of men more than those of the United States. Every step by which they have advanced to the character of an independent nation seems to have\n",
        "been distinguished by some token of providential agency. And in the important revolution just accomplished in the system of their united government,\n",
        "the tranquil deliberations and voluntary consent of so many distinct communities from which the event has resulted can not be compared with the means\n",
        "by which most governments have been established without some return of pious gratitude, along with an humble anticipation of the future blessings\n",
        "which the past seem to presage. These reflections, arising out of the present crisis, have forced themselves too strongly on my mind to be suppressed.\n",
        "You will join with me, I trust, in thinking that there are none under the influence of which the proceedings of a new and free government can more\n",
        "auspiciously commence. By the article establishing the executive department, it is made the duty of the President \"to recommend to your consideration\n",
        "such measures as he shall judge necessary and expedient.\" The circumstances under which I now meet you will acquit me from entering into that subject\n",
        "further than to refer to the great constitutional charter under which you are assembled, and which, in defining your powers, designates the objects to\n",
        "which your attention is to be given. It will be more consistent with those circumstances, and far more congenial with the feelings which actuate me,\n",
        "to substitute in place of a recommendation of particular measures the tribute that is due to the talents, the rectitude, and the patriotism which adorn\n",
        "the characters selected to devise and adopt them. In these honorable qualifications, I behold the surest pledges that as on one side no local prejudices\n",
        "or attachments, no separate views, nor party animosities will misdirect the comprehensive and equal eye which ought to watch over this great assemblage\n",
        "of communities and interests; so, on another, that the foundations of our national policy will be laid in the pure and immutable principles of private\n",
        "morality; and the preeminence of a free government be exemplified by all the attributes which can win the affections of its citizens and command the\n",
        "respect of the world.'''.split()\n",
        "\n",
        "vocab = sorted(set(raw_text))\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "wNnRYLRaQIOT",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "class model_CBOW(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(model_CBOW, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "def create_token_func():\n",
        "    global word_to_ix\n",
        "    global ix_to_word\n",
        "\n",
        "    word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "    ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
        "\n",
        "def words_to_token(words, return_as_tensor=True):\n",
        "    token_list = []\n",
        "\n",
        "    for one_word in words:\n",
        "        token_list.append(word_to_ix[one_word])\n",
        "\n",
        "    if return_as_tensor:\n",
        "        return torch.tensor(token_list, dtype=torch.long)\n",
        "    else:\n",
        "        return token_list\n",
        "\n",
        "def CBOW():\n",
        "    global input_target_words\n",
        "\n",
        "    input_target_words = []\n",
        "    for i in range(WINDOW_SIZE, len(raw_text) - WINDOW_SIZE):\n",
        "        if i + WINDOW_SIZE >= len(raw_text):\n",
        "            break\n",
        "\n",
        "        context = raw_text[i-WINDOW_SIZE:i+WINDOW_SIZE+1]\n",
        "        target = raw_text[i]\n",
        "        context.remove(target)\n",
        "        target = [target]\n",
        "        input_target_words.append((context, target))"
      ],
      "metadata": {
        "id": "CfMb1gKJ8Zkk",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "def training():\n",
        "    loss_function = nn.NLLLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "    epoch_size = 100\n",
        "\n",
        "    for epoch in range(epoch_size):\n",
        "        total_loss = 0\n",
        "\n",
        "        for context, target in input_target_words:\n",
        "            context_vector = words_to_token(context)\n",
        "            log_probs = model(context_vector)\n",
        "            t1 = words_to_token(target, return_as_tensor=False)\n",
        "            total_loss += loss_function(log_probs, torch.tensor(t1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def testing():\n",
        "    select_context_idx = 41\n",
        "    context, target = input_target_words[select_context_idx]\n",
        "    print(f'context truth: {context}')\n",
        "    print(f'target truth: {target}')\n",
        "    print(f'-------')\n",
        "\n",
        "    context_vector = words_to_token(context)\n",
        "    y_pred = model(context_vector)\n",
        "\n",
        "    print(f'Prediction: {ix_to_word[torch.argmax(y_pred[0]).item()]}')"
      ],
      "metadata": {
        "id": "c8eOz6yq8CV8",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "create_token_func()\n",
        "CBOW()\n",
        "model = model_CBOW(vocab_size, EMDEDDING_DIM)\n",
        "training()\n",
        "testing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w73ZRuAn7941",
        "outputId": "d630c07f-cbeb-4125-80bd-e1578b6b9653",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context truth: ['was', 'summoned', 'by', 'my', 'whose', 'voice', 'I', 'can']\n",
            "target truth: ['Country,']\n",
            "-------\n",
            "Prediction: Country,\n"
          ]
        }
      ],
      "execution_count": 5
    }
  ]
}